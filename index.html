<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="ActNeRF: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions">
  <meta name="keywords" content="ActNeRF, ActNeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ActNeRF</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf">ActNeRF</span>: Generalizable and Rapid Physical Task Planning with Physics-Informed Skill Networks for Robot Manipulators</h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);">......</h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Saptarshi Dasgupta<sup>*</sup>,</span>
                <span class="author-block">
                  Akshat Gupta<sup>*</sup>,</span>
                <span class="author-block">
                  Shreshth Tuli<sup></sup>,</span>
                <span class="author-block">
                  Roahn Paul<sup></sup>,</span>
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span> -->
                <!-- <span class="author-block"><sup>2</sup>Affiliated with IIT Delhi, </span> -->
                <span class="author-block"><sup>*</sup>Indicate equal contribution, </span>
                <!-- <span class="author-block"><sup>#</sup>Indicate equal advising, </span> -->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
                  <!-- <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/ActNeRF/ActNeRF"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>

  <div class="container is-max-desktop has-text-centered">
    <h2 class="subtitle" , style="max-width: 90%; padding-left: 10%;">
      <span class="dnerf">ActNeRF</span> is an approach to learn complete 3D object models using robot manipulators leveraging contact based interactions with the robot to re-orient objects to expose occluded surfaces during training.
    </h2>
  </div>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <h2 class="title has-text-centered is-3">Abstract</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          Manipulating unseen objects is challenging with-
          out a 3D representation, as objects generally have occluded
          surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use
          an ensemble of partially constructed NeRF models to quantify
          model uncertainty to determine the next action (a visual
          or re-orientation action) by optimizing informativeness and
          feasibility. Further, our approach determines when and how to
          grasp and re-orient an object given its partial NeRF model and
          re-estimates the object pose to rectify misalignments introduced
          during the interaction. Experiments with a simulated Franka
          Emika Robot Manipulator operating in a tabletop environment
          with benchmark objects demonstrate an improvement of (i)
          14% in visual reconstruction quality (PSNR), (ii) 20% in the
          geometric/depth reconstruction of the object surface (F-score)
          and (iii) 71% in the task success rate of manipulating objects
          a-priori unseen orientations/stable configurations in the scene;
          over current methods
        </p>
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/overview.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" /> 
          </div>
          <!-- Text Column -->
          </div>
        </div>
        <!-- Subtitle Section -->
        <!-- <div class="has-text-justified">
          <h2 class="subtitle">
            <span class="dnerf">ActNeRF</span> is a novel physics-informed planning framework based on accelerated
            learning of Physical Reasoning Tasks using Physics Informed Skill Networks
          </h2>
        </div> -->
      

      <div class="columns is-centered is-vcentered has-text-justified">
        <p>
          <b>ActNeRF Overview:</b> We present an active learning approach for building a NeRF model of an object that allows the robot to re-orient the object while collecting visual observations. Our approach is guided by a measure of model uncertainty in the partially constructed model, which is used to determine the most informative feasible action, aiding model construction.
        </p>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Re-orientation Pipeline</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/reorientation.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" />
          </div>
          <!-- Text Column -->
          </div>
      <div class="has-text-justified">
        <p>
          First, the RGB and Depth images are rendered from the object's current NeRF model. Using these, AnyGrasp detects potential grasps, which are then pruned based on the geometry of the generated point cloud and NeRFâ€™s material density on grasp patches. The best grasp is selected from the remaining using our uncertainty-aware grasp score. The robot executes the chosen grasp to re-orient the object, and the modified iNeRF is employed to re-acquire the object's pose in its new orientation. We show the quality of the object models before and after the flip. The post-flip model is obtained by capturing images in a re-oriented position and adding them to the training dataset
        </p>
        <br>
      </div>
      <hr>
  <hr>

    <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Re-orientation Pipeline</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/gsr.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" />
          </div>
          <!-- Text Column -->
          </div>
      <div class="has-text-justified">
        <p>
          We analyze the quality of the acquired models using a manipulation task, where the object
          is placed at a random pose in the workspace and the robot has to attempt a grasp using the 
          learned models. In the figure we show the task success
          rate and failure scenarios. Active denotes vanilla ActiveNeRF [Pan et al.] trained on
          captured images without segmentation and with no Flip() action. S
          denotes object segmentation, F denotes possibility of Flip() action
        </p>
        <br>
      </div>
      <hr>
  <hr>

  


  <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Pan et al., 2022] Pan, X., Lai, Z., Song, S., & Huang, G. (2022, October).
         Activenerf: Learning where to see with uncertainty estimation. 
         In European Conference on Computer Vision (pp. 230-246). Cham: Springer Nature Switzerland.</code></pre>
    </div>
  </section>
<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{ActNeRF2024,
      title     = {ActNeRF: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions},
      author    = {Chopra, Mudit and Barnawal, Abhinav and Vagadia, Harshil and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
