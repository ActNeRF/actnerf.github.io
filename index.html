<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="ActNeRF: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions">
  <meta name="keywords" content="ActNeRF, ActNeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ActNeRF</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><span class="dnerf">ActNeRF</span>: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions</h1>
              <h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);">......</h3>
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Saptarshi Dasgupta<sup>*</sup>,</span>
                <span class="author-block">
                  Akshat Gupta<sup>*</sup>,</span>
                <span class="author-block">
                  Shreshth Tuli<sup></sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup></sup></span>
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <!-- <span class="author-block"><sup>1</sup>Work primarily done when at IIT Delhi, </span> -->
                <!-- <span class="author-block"><sup>2</sup>Affiliated with IIT Delhi, </span> -->
                <span class="author-block"><sup>*</sup>Indicates equal contribution, </span>
                <span class="author-block"><sup></sup>Indian Institute of Technology Delhi </span>
                <!-- <span class="author-block"><sup>#</sup>Indicate equal advising, </span> -->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
                  <!-- <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/ActNeRF/ActNeRF"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  <hr>

  <div class="container is-max-desktop has-text-centered">
    <h2 class="subtitle" , style="max-width: 90%; padding-left: 10%;">
      <span class="dnerf">ActNeRF</span> is an approach to learn complete 3D object models using robot manipulators leveraging contact based interactions with the robot to re-orient objects to expose occluded surfaces during training.
    </h2>
  </div>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <h2 class="title has-text-centered is-3">Abstract</h2>
      <div class="content is-four-fifths has-text-justified">
        <p>
          Manipulating unseen objects is challenging with-
          out a 3D representation, as objects generally have occluded
          surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use
          an ensemble of partially constructed NeRF models to quantify
          model uncertainty to determine the next action (a visual
          or re-orientation action) by optimizing informativeness and
          feasibility. Further, our approach determines when and how to
          grasp and re-orient an object given its partial NeRF model and
          re-estimates the object pose to rectify misalignments introduced
          during the interaction. Experiments with a simulated Franka
          Emika Robot Manipulator operating in a tabletop environment
          with benchmark objects demonstrate an improvement of (i)
          14% in visual reconstruction quality (PSNR), (ii) 20% in the
          geometric/depth reconstruction of the object surface (F-score)
          and (iii) 71% in the task success rate of manipulating objects
          a-priori unseen orientations/stable configurations in the scene;
          over current methods
        </p>
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/overview.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" /> 
          </div>
          <!-- Text Column -->
          </div>
        </div>
        <!-- Subtitle Section -->
        <!-- <div class="has-text-justified">
          <h2 class="subtitle">
            <span class="dnerf">ActNeRF</span> is a novel physics-informed planning framework based on accelerated
            learning of Physical Reasoning Tasks using Physics Informed Skill Networks
          </h2>
        </div> -->
      

      <div class="columns is-centered is-vcentered has-text-justified">
        <p>
          <b>ActNeRF Overview:</b> We present an active learning approach for building a NeRF model of an object that allows the robot to re-orient the object while collecting visual observations. Our approach is guided by a measure of model uncertainty in the partially constructed model, which is used to determine the most informative feasible action, aiding model construction.
        </p>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">Re-orientation Pipeline</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/reorientation.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" />
          </div>
          <!-- Text Column -->
          </div>
      <div class="has-text-justified">
        <p>
          First, the RGB and Depth images are rendered from the object's current NeRF model. Using these, AnyGrasp detects potential grasps, which are then pruned based on the geometry of the generated point cloud and NeRFâ€™s material density on grasp patches. The best grasp is selected from the remaining using our uncertainty-aware grasp score. The robot executes the chosen grasp to re-orient the object, and the modified iNeRF is employed to re-acquire the object's pose in its new orientation. We show the quality of the object models before and after the flip. The post-flip model is obtained by capturing images in a re-oriented position and adding them to the training dataset
        </p>
        <br>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">ActNeRF in Action</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <!-- Image Column -->
          <div>
            <img src="./static/images/run.png"
              <!-- style="border-radius: 25px; border: 2px solid #555555;" />
          </div>
          <!-- Text Column -->
          </div>
      <div class="has-text-justified">
        <p>
          We show the RGB images and uncertainty maps rendered from trained models during our active learning process. The GT images are shown for reference. We note from the figure that before flipping, the bottom surface of the object has high uncertainty, which only diminishes once we perform the flip and acquire information about the bottom surface. The robot then uses the acquired object model to manipulate the object in any orientation.
        </p>
        <br>
      </div>
    </div>
  </section>
<!-- 
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Physical Reasoning Tasks</h2>
      <div id="tasks" , class="column has-text-justified">
        <p>
          We created the following four challenging 3D physical reasoning tasks to analyse the performance of ActNeRF,
          inspired by prior works in simplistic 2D environments presnted in <a href="https://arxiv.org/abs/1907.09620"
            class="external-link is-normal">[Allen et al., 2020]</a> and <a href="https://phyre.ai/"
            class="external-link is-normal">[Bakhtin et al., 2019]</a>.
          ActNeRF performs semantic reasoning using PINN-based Skill Models before executing each action in the
          environemnt (just as Humans think before executing). It also learns the difference between PINN-based rewards
          for actions and actual rewards as it executes actions (called online learning). Therefore, it often improves
          in subsequent actions (Just as Humans improve their actions with more trials). The videos show the actions
          taken by ActNeRF on each task. The effect of online learning is more evident in Bounce and Bridge tasks where
          the robot fails to perform well in early attempts.
        </p>
      </div>
      <hr>
      <div class="columns is-multiline">
        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Launch Task</h3>
            <p class="has-text-justified">Robot trains to use the pendulum object present in the environment to make the
              ball reach the goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/launch.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              The robot learns to correctly align the pendulum's plane and angle to throw the ball into the box.
            </p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Slide Task </h3>
            <p class="has-text-justified">Robot trains to use pendulum object present in the environment to slide the
              puck to the goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/slide.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              The following five trials represent the robot eventually sliding the puck to reach the goal by aligning
              the pendulum and using physical skills like hitting and sliding.
            </p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Bounce Task</h3>
            <p class="has-text-justified">Robot trains to use wedge object present in the environment to make the ball
              reach the goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/bounce.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              In the above five trials, the robot places the wedge at the correct location with the proper orientation,
              throwing the ball from the proper height, accomplishing the ball reaching the goal.
            </p>
          </div>
        </div>

        <div class="column is-centered is-half">
          <div class="content">
            <h3 class="title is-4 has-text-centered">Bridge Task</h3>
            <p class="has-text-justified">Robot trains to use the pendulum and bridge objects present in the environment
              to make the puck reach the
              goal</p>
            <video id="matting-video" controls playsinline height="100%" style="border-radius: 25px;">
              <source src="./static/video/bridge.mp4" type="video/mp4">
            </video>
            <p class="teaser has-text-justified">
              Over the shown trials, the robot learns to correctly align the pendulum so that the hitting plane is
              correctly aligned. Eventually, the robot effectively uses objects like the bridge present in the
              environment.
              <!-- These, along with multiple physical skills involved in the task, highlight ActNeRF's adaptability to long-horizon tasks. -->
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Comparison with the Baseline</h2>
      <div class="column has-text-justified">
        <p>
          Below is a quantitative comparison with the baseline "ActivNeRF" [Pan et. al., 2022] which integrates the uncertainty estimation into the NeRF pipeline itself. It optimizes MLE loss in addition with MSE loss used by vanilla NeRF to learn the uncertainty associated with each point in 3D space. It solves the problem of next-best-view as a purely visual problem, hence, (i) it learns the whole model of the scene instead of just the object, (ii) it does not have a notion of action costs and (iii) it doesn't consider any physical interaction with the object. While comparing we incorporate a Flip() action in ActiveNeRF for the experiments when Flip() action is allowed. 
        </p>
      </div>
      <hr>
      <h3 class="title is-4 has-text-centered"><span class="dnerf">Grasping Performance Analysis</span></h3>
      <div class="columns is-centered is-vcentered">
        <div class="column is-half has-text-justified">
          <p>
            <i>We analyze the quality of the acquired models using a manipulation task, where the object
              is placed at a random pose in the workspace and the robot has to attempt a grasp using the 
              learned models. In the figure we show the task success
              rate and failure scenarios. Active denotes vanilla ActiveNeRF [Pan et al.] trained on
              captured images without segmentation and with no Flip() action. S
              denotes object segmentation, F denotes possibility of Flip() action
          </p>
        </div>
        <!-- Image Column -->
        <div class="column ">
          <img src="./static/images/gsr.png" width="1976" height="976"
            style="border-radius: 25px; border: 2px solid #000000;" />
        </div>
      </div>
      
  


  <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Pan et al., 2022] Pan, X., Lai, Z., Song, S., & Huang, G. (2022, October).
         Activenerf: Learning where to see with uncertainty estimation. 
         In European Conference on Computer Vision (pp. 230-246). Cham: Springer Nature Switzerland.</code></pre>
    </div>
  </section>
<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{ActNeRF2024,
      title     = {ActNeRF: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions},
      author    = {Chopra, Mudit and Barnawal, Abhinav and Vagadia, Harshil and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
